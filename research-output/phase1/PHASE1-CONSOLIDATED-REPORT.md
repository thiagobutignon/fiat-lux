# Phase 1: Weight Pattern Analysis - Consolidated Report
## Llama 3.1 8B Hallucination Mechanisms Research

**Date**: 2025-10-08
**Model**: Meta Llama 3.1 8B Instruct (Q4_K_M quantization)
**Total Parameters**: 8,030,261,312
**Analysis Scope**: 32 transformer layers (96 components: 32 attention + 32 FFN + 32 norm)

---

## ğŸ¯ Executive Summary

Phase 1 weight analysis has uncovered **5 critical structural patterns** in Llama 3.1 8B that create conditions favorable for hallucinations, particularly in **Layer 31** (the final decoder layer). These patterns converge to create a "perfect storm" where:

1. **Attention mechanisms weaken** progressively (-37% from layer 0â†’31)
2. **FFN pattern-matching becomes dominant** (2x stronger than attention in layer 31)
3. **Value tensors exhibit bimodal sparsity** (alternating 2.8%/0.01%)
4. **Value amplification increases** (+134% by layer 31)
5. **Layer normalization amplifies** late-layer signals (+30.7% in layer 31)

**Key Implication**: Layer 31 generates outputs through **pattern-matching (FFN) rather than context-retrieval (attention)**, creating plausible but potentially unfactual text.

---

## ğŸ“Š Discovery #1: Value Tensor Bimodal Sparsity

### Pattern
Value tensors alternate between **high sparsity (2.79%)** and **low sparsity (0.01%)** across layers.

### Measurements
- **Layer 0**: V sparsity = 0.01% (dense)
- **Layer 15**: V sparsity = 2.79% (sparse)
- **Layer 31**: V sparsity = 2.79% (sparse)
- **Pattern**: Inconsistent, non-monotonic

### Hypothesis
**Information Bottleneck Theory**: Alternating sparsity creates inconsistent context representation, where some layers have rich context (dense V) while others have sparse, selective context. This inconsistency may cause:
- **Context drift** across layers
- **Information loss** at sparse layers
- **Retrieval errors** when downstream layers expect dense context

### Impact on Hallucinations
âš ï¸ **Medium-High**: Sparse value tensors in late layers (including layer 31) mean less contextual information is preserved for final output generation, increasing reliance on learned patterns.

---

## ğŸ“Š Discovery #2: Progressive Attention Weakening

### Pattern
Query-to-Gate ratio (attention strength vs FFN strength) **declines 37%** from early to late layers.

### Measurements
```
Layer 0:  Q/Gate = 0.79  (Attention-dominant)
Layer 10: Q/Gate = 0.70  (Balanced)
Layer 20: Q/Gate = 0.61  (Transition)
Layer 31: Q/Gate = 0.50  (FFN-dominant, 2x stronger!)
```

### Layer-by-Layer Dynamics
- **Early Layers (0-10)**: Attention dominant (Q/Gate â‰ˆ 0.75-0.79)
  - **Role**: Context gathering, factual information extraction
  - **Behavior**: Strong attention to input tokens

- **Middle Layers (11-20)**: Transition zone (Q/Gate â‰ˆ 0.63-0.70)
  - **Role**: Abstract representation building
  - **Behavior**: Weakening attention, increasing FFN influence

- **Late Layers (21-30)**: FFN dominant (Q/Gate â‰ˆ 0.57-0.62)
  - **Role**: Pattern-based text generation
  - **Behavior**: FFN pattern-matching overtakes context-awareness

- **Layer 31**: FFN 2x stronger than attention (Q/Gate = 0.50)
  - **Role**: Final output generation
  - **Behavior**: âš ï¸ **Pattern-matching without strong context verification**

### Hypothesis
**Attention Decay Theory**: As layers deepen, the model shifts from:
1. **Context-aware retrieval** (attention-driven) â†’
2. **Pattern-based generation** (FFN-driven)

This shift is **architectural** (baked into the weights), meaning layer 31 is **structurally biased** toward generating text based on learned patterns rather than verifying against context.

### Impact on Hallucinations
ğŸ”´ **Critical**: Layer 31's weak attention means the final output layer **cannot adequately verify** generated text against the input context, allowing plausible but incorrect patterns to pass through.

---

## ğŸ“Š Discovery #3: Value Amplification + Key Deterioration

### Pattern
Value tensor magnitudes **increase 134%** while Key tensor magnitudes **decrease 14%** from layer 0 to 31.

### Measurements

**Value Amplification:**
```
Layer 0:  V L2 Norm = 14.7
Layer 15: V L2 Norm = 24.5  (+67%)
Layer 31: V L2 Norm = 34.4  (+134%)
```

**Key Matching Deterioration:**
```
Layer 0:  K L2 Norm = 56
Layer 15: K L2 Norm = 52  (-7%)
Layer 31: K L2 Norm = 48  (-14%)
```

### Combined Effect
**Amplified Noise Hypothesis**:
- **High V + Low K** = Retrieves wrong context with high confidence
- **High V + High V sparsity** (2.79%) = Amplifies **sparse, selective** signals
- Result: Layer 31 strongly projects a **narrow subset** of contextual information, potentially missing crucial facts

### Mechanism
```
Attention(Q, K, V) = softmax(QÂ·K / âˆšd) Â· V

With:
- K â†“ (weaker matching)
- V â†‘ (stronger projection)
- V sparse (selective retrieval)

Result:
- Less precise matching (Kâ†“)
- Stronger amplification of matched values (Vâ†‘)
- But matched values are sparse â†’ amplifying noise!
```

### Impact on Hallucinations
ğŸ”´ **Critical**: When **weak keys** match incorrectly AND **amplified sparse values** project strongly, layer 31 generates **high-confidence outputs** based on **weakly-matched, sparse context**.

---

## ğŸ“Š Discovery #4: Layer Norm Amplification

### Pattern
Both **Attention Norm** and **FFN Norm** scales **increase** in late layers, with FFN Norm showing the strongest amplification.

### Measurements

**Attention Norm Trend:**
```
Early Layers (0-10):   Mean = 0.301
Middle Layers (11-21): Mean = 0.437  (+45%)
Late Layers (22-31):   Mean = 0.489  (+63%)
```

**FFN Norm Trend:**
```
Early Layers (0-10):   Mean = 0.243
Middle Layers (11-21): Mean = 0.340  (+40%)
Late Layers (22-31):   Mean = 0.492  (+102%)
```

**Layer 31 Specific:**
```
Attn Norm: 0.438 (+5.9% vs avg)
FFN Norm:  0.479 (+30.7% vs avg!)
```

### Hypothesis
**Signal Amplification Theory**: Layer norms in late layers are **calibrated** to:
1. **Boost weak signals** from attention (which is already weakened)
2. **Amplify strong FFN outputs** (which are already dominant)

This creates a **positive feedback loop**:
```
Weak Attention â†’ Norm tries to compensate â†’ Amplifies weak/noisy signal
Strong FFN â†’ Norm amplifies further â†’ Dominant pattern-matching
```

### Impact on Hallucinations
ğŸŸ¡ **Medium**: Layer 31's **high FFN Norm scale** (+30.7%) further strengthens FFN-driven pattern generation, making it even harder for attention to correct course.

---

## ğŸ“Š Discovery #5: Layer 31 "Perfect Storm"

### Convergence of Risk Factors

Layer 31 exhibits **ALL hallucination risk factors simultaneously**:

| Factor | Layer 31 Value | Comparison | Risk Level |
|--------|---------------|------------|-----------|
| **Q/Gate Ratio** | 0.50 | Lowest in model (-37% vs L0) | ğŸ”´ Critical |
| **Value Sparsity** | 2.79% | High sparsity | ğŸŸ¡ Medium |
| **Value Amplification** | 34.4 | Highest in model (+134%) | ğŸ”´ Critical |
| **Key Matching** | 48 | Lowest in model (-14%) | ğŸŸ¡ Medium |
| **Output Projection** | 57 | Highest in model | ğŸŸ  Moderate |
| **FFN Gate Strength** | 140 | Highest in model | ğŸ”´ Critical |
| **FFN Norm Scale** | 0.479 | 30.7% above avg | ğŸŸ¡ Medium |

### Mechanism Diagram

```
INPUT CONTEXT
     â†“
Layer 0-10: STRONG ATTENTION (Q/Gate=0.75-0.79)
     â†“ Context gathered, facts extracted
     â†“
Layer 11-20: TRANSITION (Q/Gate=0.63-0.70)
     â†“ Attention weakening, FFN rising
     â†“ Value sparsity alternates â†’ context inconsistency
     â†“
Layer 21-30: FFN DOMINANCE (Q/Gate=0.57-0.62)
     â†“ Pattern-matching overtakes context
     â†“ Value amplification increases
     â†“
Layer 31: HALLUCINATION TRIGGER
     â”œâ”€ Weak Attention (Q/Gate=0.50) â†’ poor context verification
     â”œâ”€ Weak Keys (L2=48) â†’ imprecise matching
     â”œâ”€ Sparse Values (2.79%) â†’ selective retrieval
     â”œâ”€ Amplified Values (L2=34, +134%) â†’ amplifies noise
     â”œâ”€ Strong Output (L2=57) â†’ projects confidently
     â”œâ”€ Dominant FFN (Gate=140) â†’ pattern-based generation
     â””â”€ High FFN Norm (+30.7%) â†’ amplifies FFN output
           â†“
     RESULT: HIGH-CONFIDENCE, PATTERN-BASED OUTPUT
             WITHOUT STRONG CONTEXT VERIFICATION
           â†“
     HALLUCINATION (plausible but potentially incorrect)
```

### Combined Impact
ğŸ”´ ğŸ”´ ğŸ”´ **CRITICAL**: All risk factors converge in layer 31, creating a **structural predisposition** toward generating plausible text based on learned patterns rather than verifying against input context.

---

## ğŸ“Š Discovery #6: FFN Gate Homogeneity

### Pattern
All 32 FFN gates have **near-perfect similarity** (cosine similarity â‰ˆ 1.0) in their weight distributions.

### Measurements
```
Cross-layer similarity: 1.0000 (all pairs)
Clustering: 1 cluster (all 32 layers)
Diversity: 0.0000 (no specialization detected)
```

### Hypothesis
**Lack of Specialization Theory**: Unlike attention heads which can specialize for different tasks, FFN gates show **homogeneous weight distributions**, suggesting:
1. **Runtime activation** (not weights) determines FFN behavior
2. **Uniform pattern-matching** across all layers
3. **No architectural diversity** to prevent overgeneralization

### Implication
FFN layers may rely more on **input-dependent activation** than on **specialized weight patterns**, meaning the same FFN structure can generate:
- âœ… Correct patterns (when activated by correct context)
- âŒ Incorrect patterns (when activated by weak/noisy context)

With weak attention in layer 31, FFN activates on **degraded context** â†’ generates plausible but potentially incorrect patterns.

### Impact on Hallucinations
ğŸŸ¡ **Medium**: Homogeneous FFN weights mean there's **no specialization** to detect/prevent inappropriate pattern generation in late layers.

---

## ğŸ”¬ Proposed Hallucination Mechanism

### 4-Stage Process

#### **Stage 1: Context Gathering (Layers 0-10)**
- **Strong attention** (Q/Gate â‰ˆ 0.75-0.79)
- **Factual information extracted** from input
- **Dense values** carry rich context

#### **Stage 2: Transition (Layers 11-20)**
- **Weakening attention** (Q/Gate â‰ˆ 0.63-0.70)
- **Value sparsity alternates** â†’ context inconsistency begins
- **FFN influence increases**

#### **Stage 3: Pattern Dominance (Layers 21-30)**
- **FFN overtakes attention** (Q/Gate â‰ˆ 0.57-0.62)
- **Value amplification** (+67% to +100%)
- **Pattern-based generation** begins

#### **Stage 4: Hallucination Trigger (Layer 31)**
- **Weakest attention** (Q/Gate = 0.50, FFN 2x stronger)
- **Weak keys** (L2=48, -14%) â†’ imprecise matching
- **Amplified sparse values** (L2=34, +134%, sparsity=2.79%)
- **Strongest FFN** (Gate=140) + **High FFN Norm** (+30.7%)
- **Strong output projection** (O=57)

**Result**: FFN generates plausible text **WITHOUT adequate context verification**
â†’ **HALLUCINATION** with high confidence!

---

## ğŸ“ˆ Success Metrics - Phase 1

âœ… **Completed Tasks:**
- âœ… Task 1.1: Weight statistics analysis (8.03B parameters analyzed)
- âœ… Task 1.2: Attention vs FFN comparative analysis (32 layers)
- âœ… Task 1.3: FFN gate specialization analysis (similarity matrix computed)
- âœ… Task 1.4: Layer norm investigation (32 norm layers analyzed)

âœ… **Data Generated:**
- Weight profile JSON (204KB, 96 layer components)
- FFN specialization analysis (32x32 similarity matrix)
- Layer norm statistics (64 norm tensors)
- Visualization scripts (ready for matplotlib)

âœ… **Key Discoveries:**
- 5 critical structural patterns identified
- Layer 31 "perfect storm" mechanism proposed
- Quantitative evidence for attention-FFN imbalance
- Layer norm amplification patterns revealed

---

## ğŸ¯ Next Steps - Phase 2: Activation Analysis

### Objectives
1. **Validate hypotheses** with runtime activation data
2. **Trace information flow** during actual hallucination events
3. **Measure attention weights** in layers 0 vs 31 for same prompt
4. **Test interventions**:
   - Boost attention in layer 31
   - Reduce FFN dominance
   - Adjust layer norm scales

### Experiments Needed
1. **Baseline**: Run inference with hallucination-prone prompts, capture activations
2. **Attention Intervention**: Artificially strengthen attention in layer 31, compare outputs
3. **Value Sparsity Intervention**: Force dense values in late layers, measure impact
4. **FFN Suppression**: Reduce FFN influence in layer 31, test factuality

### Expected Outcomes
If hypotheses correct:
- âœ… Strengthening layer 31 attention â†’ **reduces hallucinations**
- âœ… Dense values in late layers â†’ **better context preservation**
- âœ… Reduced FFN dominance â†’ **less pattern-based generation**

---

## ğŸ“š Research Impact

### Contributions to Field
1. **Quantitative evidence** for attention-FFN imbalance in transformer decoders
2. **Layer-specific analysis** identifying layer 31 as hallucination trigger
3. **Multi-factor convergence** theory (5 factors create "perfect storm")
4. **Structural explanation** for hallucinations (beyond training data)

### Potential Applications
1. **Model Architecture**: Design transformers with **balanced attention-FFN** in late layers
2. **Inference-Time Fixes**: Boost attention or suppress FFN in layer 31
3. **Training Objectives**: Add regularization to prevent attention decay
4. **Quantization**: Preserve attention weights in late layers during compression

---

## ğŸ”— Appendix: Data Files

### Generated Outputs
```
research-output/phase1/
â”œâ”€â”€ weight-profile-1759955262868.json          # Complete weight profile (8.03B params)
â”œâ”€â”€ task1.3-ffn-specialization-*.json          # FFN gate similarity analysis
â”œâ”€â”€ task1.4-layernorm-analysis-*.json          # Layer norm statistics
â””â”€â”€ PHASE1-CONSOLIDATED-REPORT.md              # This report
```

### Source Code
```
src/research/llama-hallucination/
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ weight-extractor.ts                    # Weight extraction engine
â”‚   â””â”€â”€ dequantizer.ts                         # Q4_K dequantization
â”œâ”€â”€ demos/
â”‚   â”œâ”€â”€ phase1-weight-analysis.ts              # Task 1.1 main script
â”‚   â”œâ”€â”€ phase1-task1.3-ffn-analysis.ts         # Task 1.3 FFN analysis
â”‚   â””â”€â”€ phase1-task1.4-layernorm-analysis.ts   # Task 1.4 norm analysis
â””â”€â”€ domain/
    â””â”€â”€ weight-statistics.ts                   # Data structures
```

---

## ğŸ“ Citations & References

### Model
- **Meta Llama 3.1 8B Instruct**
- Quantization: Q4_K_M (GGUF format)
- Total Parameters: 8,030,261,312
- Architecture: 32-layer transformer decoder

### Research Methodology
- **Weight Pattern Analysis**: Statistical analysis of 8.03B parameters
- **Layer Comparison**: Early (0-10) vs Middle (11-21) vs Late (22-31)
- **Component Analysis**: Attention, FFN, Layer Norm (96 total components)

---

**Report Generated**: 2025-10-08
**Research Phase**: Phase 1 - Weight Pattern Analysis
**Status**: âœ… Complete
**Next Phase**: Phase 2 - Activation Flow Tracing

---

*"The architecture reveals the hallucination before the inference begins."*
