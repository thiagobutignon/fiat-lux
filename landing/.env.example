# Gemini API Configuration
# Get your API key from: https://aistudio.google.com/apikey

# Required: Your Gemini API key
GEMINI_API_KEY=your_api_key_here

# Optional: Set to "false" to skip Gemini and only run simulated competitors
# ENABLE_GEMINI=true

# Optional: Set to "true" to enable local Llama via Ollama
# Requires Ollama installed and running: https://ollama.ai/
# ENABLE_LOCAL_LLAMA=false

# Optional: Ollama configuration
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.1:8b

# Optional: Set to "true" to enable vLLM (10-25x faster than Ollama)
# Requires vLLM installed and server running
# Install: pip install vllm
# Start: python -m vllm.entrypoints.openai.api_server --model MODEL_NAME --port 8000
# ENABLE_VLLM=false

# Optional: vLLM configuration (NVIDIA GPUs only)
# VLLM_BASE_URL=http://localhost:8000
# VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# Optional: Set to "true" to enable llama.cpp (Recommended for Mac M-series!)
# Fastest option for Mac M1/M2/M3/M4 with Metal acceleration
# Install: brew install llama.cpp
# Start: ./scripts/start-llamacpp-mac.sh
# ENABLE_LLAMACPP=false

# Optional: llama.cpp configuration
# LLAMACPP_BASE_URL=http://localhost:8080
# LLAMACPP_MODEL=Meta-Llama-3.1-8B-Instruct-Q4_K_M
