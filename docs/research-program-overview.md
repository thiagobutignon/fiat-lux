# Fiat Lux Research Program: Inside the Black Box

**Research Program**: Understanding and Engineering LLM Internal Mechanisms
**Timeline**: 7-12 months
**Target Venues**: arXiv, NeurIPS, ICML, ICLR
**Status**: Proposed

## Vision

This research program aims to fundamentally understand and engineer the internal mechanisms of Large Language Models, moving beyond black-box usage to **precise weight-level control** over model behavior. We seek to answer three foundational questions:

1. **How do LLMs work?** (Mechanistic understanding)
2. **What can we engineer?** (Behavioral boundaries)
3. **What should we engineer?** (AI safety and ethics)

## Research Pillars

### Pillar 1: Understanding - Mechanistic Analysis of Hallucinations

**Issue**: [#6 - Internal Mechanism Analysis of Llama 3.1](https://github.com/thiagobutignon/fiat-lux/issues/6)

**Goal**: Peer inside the "black box" to understand the neural pathways that lead to hallucinations and failure modes.

**Key Questions**:
- Where in the network do hallucinations originate?
- What weight patterns correlate with false but confident outputs?
- How do attention mechanisms flow differently in truthful vs. hallucinating generations?

**Methodology**:
1. **Weight Pattern Analysis**: Statistical characterization of all 8.03B parameters
2. **Activation Flow Tracing**: Track information flow through 32 layers
3. **Quantization Impact**: Understand Q4_K/Q6_K effects on hallucination rates
4. **Causal Intervention**: Ablation studies to establish causal relationships

**Expected Outcomes**:
- First comprehensive map of Llama 3.1's internal mechanisms
- Causal pathways from weights to hallucinations
- Identification of "gatekeeper" layers that control factuality

**Scientific Impact**: Nature/Science-level understanding of LLM internals

---

### Pillar 2: Engineering - Behavioral Modification and Determinism

**Issue**: [#7 - Behavioral Engineering and Deterministic Inference](https://github.com/thiagobutignon/fiat-lux/issues/7)

**Goal**: Push the boundaries of what behaviors can be engineered through weight modifications, including achieving perfect determinism.

**Key Questions**:
- Can we achieve bit-exact reproducible LLM inference?
- How can we improve performance through surgical weight modifications?
- What are the fundamental limits of weight-based behavior engineering?

**Methodology**:
1. **Deterministic Inference**: Fixed-point arithmetic, hardware-independent quantization
2. **Performance Engineering**: Redundancy elimination, sparsity-aware quantization
3. **Task-Specific Adaptation**: Weight overlays for specialized tasks
4. **Boundary Exploration**: Test limits of achievable behaviors

**Expected Outcomes**:
- First provably deterministic LLM system
- 20%+ performance improvements through weight engineering
- Open-source toolkit for weight modifications

**Practical Impact**: Production-ready deterministic LLMs for regulated industries

---

### Pillar 3: Safety - Constitutional AI at the Weight Level

**Issue**: [#8 - Weight-Level Constitutional AI](https://github.com/thiagobutignon/fiat-lux/issues/8)

**Goal**: Embed safety principles directly into weights, making harmful outputs mathematically impossible rather than merely discouraged.

**Key Questions**:
- Can we modify weights to make specific outputs unreachable?
- Do safety constraints compose without interfering?
- Can we provide formal guarantees about model safety?

**Methodology**:
1. **Principle Formalization**: Translate ethics into mathematical constraints
2. **Safety Layer Identification**: Find which components control safety
3. **Weight Modification**: Attention bias, FFN gating, residual correction
4. **Adversarial Testing**: 10k+ jailbreak attempts
5. **Formal Verification**: Mathematical proofs of safety properties

**Expected Outcomes**:
- Weight-encoded constitutional principles
- 99%+ jailbreak resistance
- Formal safety guarantees for specific properties

**Safety Impact**: Provably safe AI for critical applications

---

## Integration: The Complete Research Arc

```
Phase 1: UNDERSTANDING (Months 1-3)
â”œâ”€ Analyze weight patterns
â”œâ”€ Trace activation flows
â”œâ”€ Identify hallucination mechanisms
â””â”€ Map behavioral boundaries
    â†“
Phase 2: ENGINEERING (Months 4-6)
â”œâ”€ Implement deterministic inference
â”œâ”€ Optimize performance through weights
â”œâ”€ Test behavioral modification limits
â””â”€ Build weight engineering toolkit
    â†“
Phase 3: SAFETY (Months 7-10)
â”œâ”€ Embed constitutional principles
â”œâ”€ Adversarial robustness testing
â”œâ”€ Formal verification
â””â”€ Production deployment
    â†“
Phase 4: PUBLICATION (Months 11-12)
â”œâ”€ arXiv preprints (3 papers)
â”œâ”€ Conference submissions (NeurIPS, ICML, ICLR)
â”œâ”€ Open-source releases
â””â”€ Community engagement
```

## Synergies Between Pillars

### Understanding â†’ Engineering
Mechanistic understanding enables precise weight modifications:
- Knowing which layers control factuality allows targeted interventions
- Attention head specialization guides redundancy elimination
- Hallucination pathways inform safety layer selection

### Engineering â†’ Safety
Behavioral engineering techniques enable safety implementations:
- Deterministic inference ensures consistent safety behavior
- Weight modification toolkit applies to constitutional constraints
- Performance optimization allows safety with minimal cost

### Safety â†’ Understanding
Safety research deepens mechanistic understanding:
- Identifying safety layers reveals control mechanisms
- Adversarial testing exposes behavioral boundaries
- Formal verification requires precise internal models

## Technical Foundation

All three pillars build on our **GGUF Parser** infrastructure:

âœ… **Implemented** (PR #5):
- Complete binary format parsing (GGUF v3)
- Weight extraction for all 292 tensors
- Accurate Q4_K and Q6_K dequantization
- Statistical analysis framework

ðŸ”¨ **To Be Built**:
- Activation recording hooks
- Weight modification framework
- Inference engine with interventions
- Formal verification toolchain

## Expected Deliverables

### Scientific Publications
1. **Paper 1**: "Understanding Hallucinations: A Mechanistic Analysis of Llama 3.1" (20-25 pages)
2. **Paper 2**: "Engineering LLM Behavior Through Weight Modifications" (20-25 pages)
3. **Paper 3**: "Constitutional AI at the Weight Level: Formal Safety Guarantees for LLMs" (25-30 pages)

### Open-Source Releases
1. **Constitutional Llama 3.1 8B**: Model with embedded safety principles
2. **Deterministic Llama 3.1 8B**: Bit-exact reproducible inference
3. **Optimized Llama 3.1 8B**: 20% faster through weight engineering
4. **Weight Engineering Toolkit**: Production-ready library

### Datasets & Benchmarks
1. **Activation Trace Dataset**: 10k+ inference traces with labels
2. **Hallucination Test Suite**: 1000+ prompts that trigger failures
3. **Jailbreak Dataset**: 10k+ adversarial prompts
4. **Constitutional Benchmark**: Tests for 50+ ethical principles

### Infrastructure & Tools
1. **GGUF Analysis Framework**: Complete weight extraction and analysis
2. **Intervention Engine**: Real-time weight/activation modifications
3. **Safety Verification Pipeline**: Formal verification tools
4. **Visualization Dashboard**: Interactive exploration of weights and activations

## Resource Requirements

### Compute
- **GPU Hours**: 500-1000 hours (A100 or equivalent)
- **Storage**: 1-2 TB for models, traces, and intermediate results
- **Memory**: 64GB+ RAM for large-scale weight manipulation

### Human Resources
- **Primary Researcher**: Full-time for 12 months
- **AI Safety Advisor**: Consulting for constitutional principles
- **Formal Methods Expert**: Verification and proof assistance
- **Domain Experts**: Testing across specialized domains

### Financial
- **Cloud Compute**: $10k-20k
- **Storage**: $1k-2k
- **Conference Travel**: $5k-10k
- **Total Estimated Cost**: $15k-30k

## Success Metrics

### Scientific Success
- [ ] 3+ papers accepted at top venues (NeurIPS, ICML, ICLR)
- [ ] 100+ citations within 12 months
- [ ] Invited talks at major labs (Anthropic, OpenAI, DeepMind)
- [ ] Featured in distill.pub or similar venues

### Technical Success
- [ ] Deterministic inference with <0.1% performance cost
- [ ] 99%+ jailbreak resistance with <10% capability loss
- [ ] Formal proofs for 10+ safety properties
- [ ] 20%+ speedup through weight engineering

### Community Impact
- [ ] 1000+ GitHub stars on toolkit
- [ ] 10+ derivative research projects
- [ ] Adoption by at least one major AI lab
- [ ] Influence on AI safety standards/regulations

## Risk Mitigation

### Technical Risks
| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Weight modifications break model | Medium | High | Gradual changes, extensive testing |
| Formal verification intractable | High | Medium | Focus on empirical certification |
| Jailbreaks adapt to defenses | High | High | Continuous red teaming |
| Capabilities degrade too much | Medium | High | Multi-objective optimization |

### Research Risks
| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Results don't generalize | Medium | High | Test on multiple models/scales |
| Findings not novel enough | Low | High | Extensive literature review |
| Can't publish due to safety concerns | Low | Medium | Responsible disclosure process |

## Timeline & Milestones

### Q1 (Months 1-3): Understanding Phase
- [ ] **M1**: Weight extraction complete for all layers
- [ ] **M2**: Activation recording infrastructure built
- [ ] **M3**: Hallucination mechanisms identified
- [ ] **Deliverable**: Technical report on mechanistic findings

### Q2 (Months 4-6): Engineering Phase
- [ ] **M4**: Deterministic inference achieved
- [ ] **M5**: Performance engineering framework complete
- [ ] **M6**: Behavioral boundary map finished
- [ ] **Deliverable**: Open-source weight engineering toolkit

### Q3 (Months 7-9): Safety Phase
- [ ] **M7**: Constitutional principles embedded
- [ ] **M8**: Adversarial testing complete (99%+ resistance)
- [ ] **M9**: Formal verification for 10+ properties
- [ ] **Deliverable**: Constitutional Llama 3.1 model release

### Q4 (Months 10-12): Publication & Deployment
- [ ] **M10**: 3 papers submitted to arXiv
- [ ] **M11**: Conference submissions (NeurIPS, ICML)
- [ ] **M12**: Production deployment + community engagement
- [ ] **Deliverable**: Complete research program results

## Related Work & Positioning

### Mechanistic Interpretability
- **Anthropic (2023)**: Causal scrubbing, circuit discovery
- **Neel Nanda (2023)**: TransformerLens, mechanistic analysis
- **Our Contribution**: First comprehensive analysis of hallucination mechanisms at weight level

### Constitutional AI
- **Anthropic (2022)**: RLHF with constitutional principles
- **OpenAI (2023)**: Rule-based safety systems
- **Our Contribution**: Weight-level encoding vs. behavioral training

### LLM Optimization
- **SmoothQuant (2023)**: Activation smoothing for quantization
- **GPTQ (2023)**: Post-training quantization
- **Our Contribution**: Weight engineering for behavior, not just compression

## Long-Term Vision

This research program is the foundation for a new paradigm: **Precise LLM Engineering**

### 5-Year Horizon
- Weight-level control becomes standard practice
- Constitutional AI encoded by default in all models
- Formal verification required for safety-critical deployments
- New field: "Neural Weight Engineering"

### 10-Year Horizon
- Programmable neural networks with guaranteed behaviors
- AI systems with provable safety properties
- End of "black box" AI era
- Foundation for aligned AGI

## Call to Action

This research program represents a fundamental shift in how we understand and control AI systems. We invite:

- **Researchers**: Collaborate on mechanistic interpretability
- **Engineers**: Build on our weight engineering toolkit
- **Safety Experts**: Help define constitutional principles
- **Funders**: Support this foundational research
- **Policymakers**: Prepare for provably safe AI

---

**Contact**: thiagobutignon/fiat-lux
**Issues**: [#6](https://github.com/thiagobutignon/fiat-lux/issues/6), [#7](https://github.com/thiagobutignon/fiat-lux/issues/7), [#8](https://github.com/thiagobutignon/fiat-lux/issues/8)
**Foundation**: GGUF Parser (PR #5)

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
